{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Contrastive Divergence Assignment 4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srinivasrdhkrshnn/CS6910-Assignment-4/blob/main/Contrastive_Divergence_Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-XD0C0fT6LH"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk1JX9Z8OvrK"
      },
      "source": [
        "***Importing Libraries and Load Data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tH8IIFC4UMPg"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.datasets import mnist\n",
        "from sklearn.manifold import TSNE\n",
        "import wandb\n",
        "\n",
        "# load dataset\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "class_type = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'] \n",
        "\n",
        "proj_name='CS6910 ASSIGNMENT 4'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JBDFxo1OjPA"
      },
      "source": [
        "***Modify and Restructure Data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWyanHGZYK_5"
      },
      "source": [
        "# Initializations\n",
        "\n",
        "# Data\n",
        "X_train = np.array(x_train.reshape(x_train.shape[0], 784,1))         # reshape 2-D data to 1-D\n",
        "X_test  = np.array(x_test.reshape(x_test.shape[0], 784,1))           # reshape 2-D data to 1-D\n",
        "X_train = (X_train > 126) * 1                                        # convert the real valued data into binary data, using a threshold of 127\n",
        "X_test  = (X_test > 126) * 1                                         # convert the real valued data into binary data, using a threshold of 127\n",
        "\n",
        "X_val = X_train[-15000:]                                             # validation set input (to train Classifier)\n",
        "X_train = X_train[0:45000]                                           # training set input (to train RBM)\n",
        "\n",
        "Y_train = np.zeros([len(y_train),10,1])\n",
        "Y_test = np.zeros([len(y_test),10,1])\n",
        "\n",
        "for i in range(len(y_train)):                                        # convert y from just a class number to one hot vector (10x1)\n",
        "  y = np.zeros([10, 1])\n",
        "  y[y_train[i]] = 1.0\n",
        "  Y_train[i] = y\n",
        "\n",
        "for i in range(len(y_test)):                                         # convert y from just a class number to one hot vector (10x1)\n",
        "  y = np.zeros([10, 1])\n",
        "  y[y_test[i]] = 1.0\n",
        "  Y_test[i] = y                                                      # test set output\n",
        "\n",
        "Y_val = Y_train[-15000:]                                             # validation set output\n",
        "Y_train = Y_train[0:45000]                                           # training set output"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMIBhHkmOcLU"
      },
      "source": [
        "***Hyperparameters and Basic variables***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA9IZFltcaJq"
      },
      "source": [
        "n_visible = X_train.shape[1]                                         # number of visible neurons\n",
        "n_train_examples = X_train.shape[0]                                  # number of training data\n",
        "n_val_examples = X_val.shape[0]                                      # number of validation data\n",
        "n_test_examples = X_test.shape[0]                                    # number of test data\n",
        "n_hidden = 80                                                        # number of hidden neurons\n",
        "n_class = 10\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5jcNzIFzF4-"
      },
      "source": [
        "***Parameter Initialization (Weights and Biases)***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6hjDjNRc1FR"
      },
      "source": [
        "def rbm_param_init() :                                                                        # Function to randomly initialize the RBM network parameters \n",
        "\t\n",
        "  rbm_parameters = {}\n",
        "  rbm_parameters[\"W\"] = np.random.randn(n_hidden, n_visible)*np.sqrt(6./(n_visible + n_hidden))   # Xavier Initialization of weights\n",
        "  rbm_parameters[\"h_bias\"] = np.zeros((n_hidden,1),dtype=np.float64)                                                     \n",
        "  rbm_parameters[\"v_bias\"] = np.zeros((n_visible,1),dtype=np.float64)\n",
        "\n",
        "  return rbm_parameters\n",
        "\n",
        "def classifier_param_init() :                                                                 # Function to randomly initialize the Classifier network parameters\n",
        "\n",
        "  classifier = {}\n",
        "  classifier[\"W\"] = np.random.randn(n_class, n_hidden)*np.sqrt(6./(n_class + n_hidden))       # Xavier Initialization of weights\n",
        "  classifier[\"b\"] = np.zeros((n_class,1),dtype=np.float64)\n",
        "\n",
        "  return classifier\n",
        "\n",
        "def sigmoid(x) :                                                                              # RBM hidden layer activation function \n",
        "\t\n",
        "  return 1.0/(1.0+np.exp(-x))\n",
        "\n",
        "def softmax(x):                                                                               # Output activation function\n",
        "    \n",
        "  return np.exp(x) / np.sum(np.exp(x))  "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSGgVr3ZO8-a"
      },
      "source": [
        "***RBM and Classifier Trainer***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKRIg5kXoNlj"
      },
      "source": [
        "def rbm_train(X_train,parameters,k,learning_rate) :                                     # Function to train the RBM\n",
        "\t\n",
        "  W = parameters[\"W\"]\n",
        "  h_bias = parameters[\"h_bias\"]\n",
        "  v_bias = parameters[\"v_bias\"]\n",
        "  \n",
        "  #batches = n_examples // batchsize\n",
        "  \n",
        "  for i in range(n_train_examples) :\n",
        "      v_sample = X_train[i]\n",
        "      v_init   = X_train[i]\n",
        "\n",
        "      for t in range(k) :                                                               # Markov Chain - Loop\n",
        "         h_given_v = sigmoid(np.dot(W,v_sample)+h_bias)                                  # Evaluate p(h|v)\n",
        "         h_sample = np.random.binomial(1,h_given_v)                                      # Convert to 0's and 1's assuming binomial distribution p(h|v) \n",
        "         v_given_h = sigmoid(np.dot(np.transpose(W),h_sample)+v_bias)                    # Evaluate p(v|h)\n",
        "         v_sample = np.random.binomial(1,v_given_h)                                      # Convert to 0's and 1's assuming binomial distribution p(v|h)\n",
        "\n",
        "      # Update Rule\n",
        "      W = W + learning_rate*(np.dot(sigmoid(np.dot(W,v_init)+h_bias),np.transpose(v_init)) - np.dot(sigmoid(np.dot(W,v_sample)+h_bias),np.transpose(v_sample)))\n",
        "      v_bias = v_bias + learning_rate*(v_init-v_sample)\n",
        "      h_bias = h_bias + learning_rate*(sigmoid(np.dot(W,v_init)+h_bias) - sigmoid(np.dot(W,v_sample)+h_bias))\n",
        "\n",
        "  parameters[\"W\"] = W\n",
        "  parameters[\"h_bias\"] = h_bias\n",
        "  parameters[\"v_bias\"] = v_bias\n",
        "  print(\"Training Complete\")\n",
        "\n",
        "  return parameters\n",
        "\n",
        "\n",
        "def get_hidden(x,parameters) :                                                                # function to get the hidden representation of the test data\n",
        "   \n",
        "    W = parameters[\"W\"]\n",
        "    h_bias = parameters[\"h_bias\"]\n",
        "    hidden_prob = sigmoid(np.dot(W,x)+h_bias)\n",
        "    hidden_rep = np.random.binomial(1,hidden_prob)\n",
        "    \n",
        "    return hidden_rep\n",
        "\n",
        "def classifier_train(X,Y,rbm_parameters,classifier_param,classifier_epochs,learning_rate)  :                                  # Function to train Classifier\n",
        "    \n",
        "    W = classifier_param[\"W\"]\n",
        "    b = classifier_param[\"b\"]\n",
        "    \n",
        "    for epoch in range(classifier_epochs) :\n",
        "      for i in range(n_val_examples) :\n",
        "         # feed forward\n",
        "         hidden_rep = get_hidden(X[i],rbm_parameters)                                         # Obtain hidden representation for the given sample\n",
        "         pre_output = np.dot(W,hidden_rep)+b\n",
        "         y_hat = softmax(pre_output)\n",
        "         # backpropogate\n",
        "         dW = np.dot(-(Y[i]-y_hat),np.transpose(hidden_rep))\n",
        "         db = -(Y[i]-y_hat)\n",
        "         # Update Classifier weights\n",
        "         W = W - learning_rate*dW\n",
        "         b = b - learning_rate*db\n",
        "\n",
        "    classifier_param[\"W\"] = W\n",
        "    classifier_param[\"b\"] = b \n",
        "    \n",
        "    return classifier_param   \n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPOct2Jubjwi"
      },
      "source": [
        "def rbm_classifier(X_train,Y_train,X_val,Y_val,X_test,Y_test,rbm_epochs,classifier_epochs,k,n_hidden,learning_rate) :\n",
        "\n",
        "  classifier_param = classifier_param_init() \n",
        "  rbm_parameters = rbm_param_init()\n",
        "\n",
        "  for j in range(rbm_epochs) :\n",
        "\n",
        "     rbm_parameters = rbm_train(X_train,rbm_parameters,k,learning_rate)                             # Train RBM for one epoch\n",
        "     classifier_param = classifier_train(X_val,Y_val,rbm_parameters,classifier_param,classifier_epochs,learning_rate)               # Train the classifier with hidden representation of RBM\n",
        "     \n",
        "     accuracy = 0.0\n",
        "     loss = 0.0 \n",
        "     # Evaluate accuracy and loss over test data\n",
        "\n",
        "     for i in range(n_test_examples) :\n",
        "\n",
        "        h = get_hidden(X_test[i],rbm_parameters)\n",
        "        y_hat = softmax(np.dot(classifier_param[\"W\"],h)+classifier_param[\"b\"])\n",
        "\n",
        "        if y_hat.argmax()==Y_test[i].argmax():\n",
        "            accuracy = accuracy + 1\n",
        "        loss = loss + -1*np.sum(np.multiply(y,np.log(y_hat)))\n",
        "\n",
        "     accuracy = accuracy/n_test_examples\n",
        "     loss = loss/n_test_examples \n",
        "     print(\"Epoch : \" + str(j)+\" Accuracy : \"+ str(accuracy)+\" Loss : \"+str(loss))\n",
        "    # wandb.log({\"Accuracy\":accuracy,\"Loss\":loss,\"Epoch\":j})\n",
        "\n",
        "  return rbm_parameters,classifier_param\n",
        "      \n",
        "\n",
        "\t"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj2gP0OWUK8r"
      },
      "source": [
        "def train():\n",
        "  hyperparameter_defaults=dict(\n",
        "        rbm_epochs = 20,\n",
        "        classifier_epochs = 1,\n",
        "        k = 2,\n",
        "        n_hidden = 256,\n",
        "        learning_rate = 0.001                                                                    \n",
        "        )\n",
        "\n",
        "  wandb.init(config=hyperparameter_defaults)\n",
        "\n",
        "  config=wandb.config\n",
        "  rbm_parameters,classifier_param = rbm_classifier(X_train,Y_train,X_val,Y_val,X_test,Y_test,config.rbm_epochs,config.classifier_epochs,config.k,config.n_hidden,config.learning_rate)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "addT0XZjvWC0"
      },
      "source": [
        "def sweeper(sweep_config,proj_name):\n",
        "  sweep_id = wandb.sweep(sweep_config,project=proj_name,entity='cs6910krsrd',)\n",
        "  wandb.agent(sweep_id,train,project=proj_name,entity='cs6910krsrd',)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckwNuoUM2nbN"
      },
      "source": [
        "#sweep dictionary\n",
        "sweep_config={\n",
        "    'method':'bayes',\n",
        "    'metric':{\n",
        "        'name':'accuracy',\n",
        "        'goal':'maximize'},\n",
        "\n",
        "}\n",
        "\n",
        "parameters_dict={\n",
        "    \n",
        "    'rbm_epochs':{\n",
        "      'values':[20]  \n",
        "    },\n",
        "    'classifier_epochs':{\n",
        "        'values':[1]\n",
        "    },\n",
        "    'k':{\n",
        "        'values':[20]\n",
        "    },\n",
        "    'n_hidden':{\n",
        "        'values':[64]      #,128,256]\n",
        "    },\n",
        "    'learning_rate':{\n",
        "        'values':[0.01]\n",
        "    }\n",
        "    \n",
        "}\n",
        "\n",
        "sweep_config['parameters']=parameters_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AgFpMnsL9xpx"
      },
      "source": [
        "sweeper(sweep_config,proj_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IL7Wk_ASjoy"
      },
      "source": [
        "***Training over Best Model***\n",
        "\n",
        "We train the RBM-Classifier Model over the estimated best hyperparameters (obtained from WandB sweep) and present the evolution of Accuracy and Loss over each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iVXhT-gzFy7",
        "outputId": "fc225551-d579-4caf-9617-34a476ac8b65"
      },
      "source": [
        "rbm_epochs = 20\n",
        "classifier_epochs = 1\n",
        "k = 2\n",
        "n_hidden = 256\n",
        "learning_rate = 0.001\n",
        "\n",
        "rbm_parameters,classifier_param = rbm_classifier(X_train,Y_train,X_val,Y_val,X_test,Y_test,rbm_epochs,classifier_epochs,k,n_hidden,learning_rate)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Complete\n",
            "Epoch : 0 Accuracy : 0.6849 Loss : 4.870453471778487\n",
            "Training Complete\n",
            "Epoch : 1 Accuracy : 0.713 Loss : 5.486611732928418\n",
            "Training Complete\n",
            "Epoch : 2 Accuracy : 0.7291 Loss : 5.8243989728626\n",
            "Training Complete\n",
            "Epoch : 3 Accuracy : 0.7407 Loss : 6.002129441648066\n",
            "Training Complete\n",
            "Epoch : 4 Accuracy : 0.7478 Loss : 6.166802580279815\n",
            "Training Complete\n",
            "Epoch : 5 Accuracy : 0.7533 Loss : 6.371643657351553\n",
            "Training Complete\n",
            "Epoch : 6 Accuracy : 0.7583 Loss : 6.486024846549208\n",
            "Training Complete\n",
            "Epoch : 7 Accuracy : 0.7621 Loss : 6.610953379689579\n",
            "Training Complete\n",
            "Epoch : 8 Accuracy : 0.7685 Loss : 6.776810180748981\n",
            "Training Complete\n",
            "Epoch : 9 Accuracy : 0.7717 Loss : 6.73076104811245\n",
            "Training Complete\n",
            "Epoch : 10 Accuracy : 0.7698 Loss : 6.900253669781621\n",
            "Training Complete\n",
            "Epoch : 11 Accuracy : 0.7728 Loss : 7.01920116076411\n",
            "Training Complete\n",
            "Epoch : 12 Accuracy : 0.7739 Loss : 7.102935901501887\n",
            "Training Complete\n",
            "Epoch : 13 Accuracy : 0.7785 Loss : 7.203392157348877\n",
            "Training Complete\n",
            "Epoch : 14 Accuracy : 0.7837 Loss : 7.227531511863465\n",
            "Training Complete\n",
            "Epoch : 15 Accuracy : 0.7784 Loss : 7.285909916687553\n",
            "Training Complete\n",
            "Epoch : 16 Accuracy : 0.7786 Loss : 7.37956366106169\n",
            "Training Complete\n",
            "Epoch : 17 Accuracy : 0.7847 Loss : 7.422668942104751\n",
            "Training Complete\n",
            "Epoch : 18 Accuracy : 0.7827 Loss : 7.477968309073709\n",
            "Training Complete\n",
            "Epoch : 19 Accuracy : 0.781 Loss : 7.494922577615496\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m86g7tgTRAGV"
      },
      "source": [
        "\n",
        "***Contrastive Divergence Results***\n",
        "\n",
        "For the best RBM-Classifier Model, the hyperparameters are \n",
        "    \n",
        "\n",
        "*   n_hidden neurons = 256\n",
        "*   number of Markov steps = 2\n",
        "*   learning rate = 0.01\n",
        "\n",
        "The accuracy achieved over test-data is 78.1% after 20 epochs of training RBM-Classifier Model.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASfxbHLoyKGT"
      },
      "source": [
        "***Visualising Hidden Representation using t-SNE Plot*** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xyC6d0mYA89"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUWWZfKkC6j8"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "hidden_reps = []\n",
        "\n",
        "for i in range(n_test_examples) :\n",
        "  \n",
        "    h = get_hidden(X_test[i],rbm_parameters)\n",
        "    hidden_reps.append(np.array(h))\n",
        "\n",
        "print(np.shape(hidden_reps))\n",
        "\n",
        "hidden_reps = np.array(hidden_reps)\n",
        "hidden_reps = np.resize(hidden_reps, (n_test_examples,256))\n",
        "embedded = TSNE(n_components = 2, early_exaggeration = 7).fit_transform(hidden_reps)           # n_components = Dimension of the embedded space (here 2D).\n",
        "\n",
        "vis_x = embedded[:,0]\n",
        "vis_y = embedded[:,1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19114e7kEpZM"
      },
      "source": [
        "run = wandb.init(project=proj_name,entity='cs6910krsrd',)\n",
        "\n",
        "\n",
        "plt.figure(figsize = (5,4))\n",
        "plt.title(\"Embeddings for testing examples\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.scatter(vis_x[:10000], vis_y[:10000], c = y_test[:10000], cmap = plt.cm.get_cmap(\"jet\", 10))\n",
        "plt.colorbar(ticks = range(max(y_test)))\n",
        "wandb.log({\"visualisation\": plt})\n",
        "plt.show()\n",
        "class_type = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "run.finish()\n",
        "     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISqEPYrPPA5i"
      },
      "source": [
        "run = wandb.init(project=proj_name,entity='cs6910krsrd',)\n",
        "\n",
        "print(\"Sample Images for each Class :\")\n",
        "class_list=list()\n",
        "\n",
        "for i in range(10):\n",
        "  plt.subplot(2,5,i+1)\n",
        "  for j in range(len(y_train)):\n",
        "    if y_train[j] == i :\n",
        "        wandb.log({\"Sample Images\": [wandb.Image(x_train[j], caption=class_type[y_train[j]])]})\n",
        "        class_list.append(class_type[y_train[j]])\n",
        "        break\n",
        "\n",
        "run.finish()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urdAk45ITm30"
      },
      "source": [
        "***Visualise the samples generated from the distribution***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xTpplLYTlMO"
      },
      "source": [
        "  run = wandb.init(project=proj_name,entity='cs6910krsrd',)\n",
        "\n",
        "\n",
        "  rbm_parameters = rbm_param_init()\n",
        "  rbm_epochs = 10\n",
        "\n",
        "  W = rbm_parameters[\"W\"]\n",
        "  h_bias = rbm_parameters[\"h_bias\"]\n",
        "  v_bias = rbm_parameters[\"v_bias\"]\n",
        "\n",
        "  for j in range(rbm_epochs) :\n",
        "\n",
        "      if (j==0) or (j==9) :\n",
        "          plt.figure(figsize = (10,8))\n",
        "          subplot_no = 1\n",
        "          \n",
        "      for i in range(n_train_examples) :\n",
        "          v_sample = X_train[i]\n",
        "          v_init   = X_train[i]\n",
        "\n",
        "          if ((j==0) or (j==9)) and ((i%(701) == 0)) and (subplot_no <= 64):\n",
        "\n",
        "            hidden = np.random.binomial(1,sigmoid(np.dot(W,v_sample)+h_bias))\n",
        "            img_reconstruct = np.random.binomial(1,sigmoid(np.dot(np.transpose(W),hidden)+v_bias))\n",
        "            img = np.reshape(img_reconstruct, (28,28))\n",
        "            plt.subplot(8,8,subplot_no)\n",
        "            plt.imshow(img, cmap = \"gray\")\n",
        "            plt.axis(\"off\")\n",
        "            subplot_no += 1\n",
        "\n",
        "          for t in range(k) :                                                                # Markov Chain - Loop\n",
        "             h_given_v = sigmoid(np.dot(W,v_sample)+h_bias)                                  # Evaluate p(h|v)\n",
        "             h_sample = np.random.binomial(1,h_given_v)                                      # Convert to 0's and 1's assuming binomial distribution p(h|v) \n",
        "             v_given_h = sigmoid(np.dot(np.transpose(W),h_sample)+v_bias)                    # Evaluate p(v|h)\n",
        "             v_sample = np.random.binomial(1,v_given_h)                                      # Convert to 0's and 1's assuming binomial distribution p(v|h)\n",
        "\n",
        "          # Update Rule\n",
        "          W = W + learning_rate*(np.dot(sigmoid(np.dot(W,v_init)+h_bias),np.transpose(v_init)) - np.dot(sigmoid(np.dot(W,v_sample)+h_bias),np.transpose(v_sample)))\n",
        "          v_bias = v_bias + learning_rate*(v_init-v_sample)\n",
        "          h_bias = h_bias + learning_rate*(sigmoid(np.dot(W,v_init)+h_bias) - sigmoid(np.dot(W,v_sample)+h_bias))\n",
        "\n",
        "\n",
        "      if (j==0) or (j==9) :\n",
        "        wandb.log({\"Sample Visualisation\": plt})\n",
        "\n",
        "\n",
        "      plt.show()\n",
        "\n",
        "  rbm_parameters[\"W\"] = W\n",
        "  rbm_parameters[\"h_bias\"] = h_bias\n",
        "  rbm_parameters[\"v_bias\"] = v_bias\n",
        "  print(\"Training Complete\")\n",
        "\n",
        "  run.finish()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}